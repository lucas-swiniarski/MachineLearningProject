{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = \"../../../Google Drive/Data_science/NYU/Machine Learning/ML Project (Collisions)/\" #Joe\n",
    "path = \"../../../../Google Drive/ML Project (Collisions)/\" # Joyce\n",
    "# path = \"\" # Lucas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + \"NYPD_Motor_Vehicle_Collisions.csv\", parse_dates=[['DATE', 'TIME']], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['DATE_TIME'] = pd.to_numeric(df['DATE_TIME'])\n",
    "df = df.sort_values('DATE_TIME', axis=0, ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "for file in os.listdir(path+\"data_for_joining/\"):\n",
    "    if file.endswith(\".pkl\") and \"injury_avgs\" not in file:\n",
    "        with open(os.path.join(path+\"data_for_joining/\", file), 'rb') as pkl_file:\n",
    "            datasets[file.replace(\".pkl\", \"\")] = pickle.load(pkl_file, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, value in datasets.items() :\n",
    "    if \"traffic\" in key:\n",
    "        datasets[key].name = datasets[key].name + '_' + key.split('_')[-1]\n",
    "    df = df.join(datasets[key], on='UNIQUE KEY', how='left', rsuffix=\"_\"+key)\n",
    "    print (\"%s finished!\" % (key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.set_index('UNIQUE KEY', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace values with inferred lat long values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inferred_datasets = {}\n",
    "\n",
    "for file in os.listdir(path+\"inferred_lat_long_data/\"):\n",
    "    if file.endswith(\".pkl\"):\n",
    "        with open(os.path.join(path+\"inferred_lat_long_data/\", file), 'rb') as pkl_file:\n",
    "            inferred_datasets[file.replace(\".pkl\", \"\")] = pickle.load(pkl_file, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, value in inferred_datasets.items():\n",
    "    try:\n",
    "        replace_columns = inferred_datasets[key].columns.values\n",
    "    except:\n",
    "        replace_columns = inferred_datasets[key].name\n",
    "        \n",
    "    df.loc[inferred_datasets[key].index, replace_columns] = inferred_datasets[key]\n",
    "    print (\"%s finished!\" % (key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "renamed_columns = [x.lower().replace(' ', '_').replace('_bin', '') for x in df.columns.values]\n",
    "    \n",
    "df.columns = renamed_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.ix[df.longitude[df.longitude < -100].index,'longitude'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "df['conditions'].value_counts().plot(kind='bar')\n",
    "plt.ylim([0, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remap_weather = {'Heavy Snow': 'Snow',\n",
    "                'Light Freezing Rain': 'Light Rain',\n",
    "                'Heavy Thunderstorms and Rain': 'Thunderstorms and Rain',\n",
    "                'Light Thunderstorms and Rain': 'Thunderstorms and Rain',\n",
    "                'Light Ice Pellets': 'Ice Pellets',\n",
    "                'Heavy Ice Pellets': 'Ice Pellets',\n",
    "                'Thunderstorms with Small Hail': 'Thunderstorms and Rain',\n",
    "                'Shallow Fog': 'Fog',\n",
    "                'Light Freezing Fog': 'Fog',\n",
    "                'Mist': 'Fog',\n",
    "                'Blowing Snow': 'Snow',\n",
    "                'Drizzle': 'Light Rain',\n",
    "                'Widespread Dust': 'Haze',\n",
    "                'Squalls': np.nan,\n",
    "                'Unknown': np.nan,\n",
    "                'Light Drizzle': 'Light Rain',\n",
    "                'Light Freezing Drizzle': 'Light Rain',\n",
    "                'Patches of Fog': 'Fog',\n",
    "                'Light Rain Showers': 'Light Rain',\n",
    "                }\n",
    "\n",
    "df['conditions'] = df['conditions'].replace(remap_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "df['conditions'].value_counts().plot(kind='bar')\n",
    "plt.ylim([0, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "df['vehicle_type_code_1'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remap_vehicle = {'TAXI': 'SMALL COM VEH(4 TIRES)',\n",
    "                'BUS': 'LARGE VEHICLE',\n",
    "                'SCOOTER': 'MOTORCYCLE',\n",
    "                'LIVERY VEHICLE': 'SMALL COM VEH(4 TIRES)',\n",
    "                'VAN': 'MEDIUM VEHICLE',\n",
    "                'PEDICAB': 'BICYCLE',\n",
    "                'PICK-UP TRUCK': 'MEDIUM VEHICLE',\n",
    "                'SPORT UTILITY / STATION WAGON': 'MEDIUM VEHICLE',\n",
    "                'LARGE COM VEH(6 OR MORE TIRES)': 'LARGE VEHICLE',\n",
    "                'FIRE TRUCK': 'LARGE VEHICLE',\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in ['vehicle_type_code_1', 'vehicle_type_code_2',\n",
    "       'vehicle_type_code_3', 'vehicle_type_code_4', 'vehicle_type_code_5']:\n",
    "    df[column] = df[column].replace(remap_vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "df['vehicle_type_code_1'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['rise_time'] = pd.to_datetime(df.rise_time).dt.hour*60 + pd.to_datetime(df.rise_time).dt.minute\n",
    "df['set_time'] = pd.to_datetime(df.set_time).dt.hour*60 + pd.to_datetime(df.set_time).dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "floats = ['latitude', 'longitude','temperature', 'heat_index', 'dew_point', 'humidity', 'pressure',\\\n",
    "          'visibility', 'wind_speed', 'gust_speed', 'precip', 'zip_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in floats:\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(path+\"data_for_training/v4/predrop_collisions.pkl\", 'wb') as outfile:\n",
    "    pickle.dump(df, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_columns = ['location', 'on_street_name', 'cross_street_name',\n",
    "       'off_street_name', 'number_of_persons_injured',\n",
    "       'number_of_persons_killed', 'number_of_pedestrians_injured',\n",
    "       'number_of_pedestrians_killed', 'number_of_cyclist_injured',\n",
    "       'number_of_cyclist_killed', 'number_of_motorist_injured',\n",
    "       'number_of_motorist_killed', 'contributing_factor_vehicle_1',\n",
    "       'contributing_factor_vehicle_2', 'contributing_factor_vehicle_3',\n",
    "       'contributing_factor_vehicle_4', 'contributing_factor_vehicle_5',\n",
    "        'injured', 'killed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fill_nas(df):\n",
    "    \n",
    "    for column in df:\n",
    "        if np.sum(df[column].isnull()) > 0:\n",
    "            df[column+'_nan'] = df[column].isnull()\n",
    "\n",
    "            if column == 'zip_code':\n",
    "                df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "            elif column in df._get_numeric_data().columns.values:\n",
    "                df[column].fillna(df[column].mean(), inplace=True)\n",
    "                \n",
    "    return df\n",
    "            \n",
    "df = fill_nas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onehotdf = pd.get_dummies(df, columns=['borough', 'wind_dir', 'conditions', 'lat/long_infered',\n",
    "                                      'vehicle_type_code_1', 'vehicle_type_code_2', 'vehicle_type_code_3',\n",
    "                                      'vehicle_type_code_4', 'vehicle_type_code_5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "renamed_columns = [x.lower().replace(' ', '_') for x in onehotdf.columns.values]\n",
    "    \n",
    "onehotdf.columns = renamed_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(path+\"data_for_training/v4/collisions_1hot.pkl\", 'wb') as outfile:\n",
    "    pickle.dump(onehotdf, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(path+\"data_for_training/v4/collisions_no1hot.pkl\", 'wb') as outfile:\n",
    "    pickle.dump(df, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "view_date = pd.to_datetime(df['date_time'])\n",
    "\n",
    "train_indices = (0, np.sum(view_date < datetime.date(2015,9,12))-1)\n",
    "val_indices = (train_indices[1]+1,\\\n",
    "               train_indices[1] + \\\n",
    "               np.sum((view_date >= datetime.date(2015,9,12)) & (view_date < datetime.date(2016,7,31))))\n",
    "test_indices = (val_indices[1]+1,\\\n",
    "               val_indices[1] + np.sum(view_date >= datetime.date(2016,7,31)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = df.iloc[train_indices[0]:train_indices[1]]\n",
    "val = df.iloc[val_indices[0]:val_indices[1]]\n",
    "test = df.iloc[test_indices[0]:test_indices[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_count = np.sum(train['num_bicycle'] > 0)\n",
    "val_count = np.sum(val['num_bicycle'] > 0)\n",
    "test_count = np.sum(test['num_bicycle'] > 0)\n",
    "total = train_count + val_count + test_count\n",
    "\n",
    "print (\"%.2f, %.2f, %.2f\" % (train_count/total*100, val_count/total*100, test_count/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_count = np.sum((train['num_vehicles'] == 1) & (train['num_bicycle'] == 0))\n",
    "val_count = np.sum((val['num_vehicles'] == 1) & (val['num_bicycle'] == 0))\n",
    "test_count = np.sum((test['num_vehicles'] == 1) & (test['num_bicycle'] == 0))\n",
    "total = train_count + val_count + test_count\n",
    "\n",
    "print (\"%.2f, %.2f, %.2f\" % (train_count/total*100, val_count/total*100, test_count/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_count = np.sum((train['num_vehicles'] == 0))\n",
    "val_count = np.sum((val['num_vehicles'] == 0))\n",
    "test_count = np.sum((test['num_vehicles'] == 0))\n",
    "total = train_count + val_count + test_count\n",
    "\n",
    "print (\"%.2f, %.2f, %.2f\" % (train_count/total*100, val_count/total*100, test_count/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_count = np.sum((train['num_vehicles'] > 1) & (train['num_bicycle'] == 0))\n",
    "val_count = np.sum((val['num_vehicles'] > 1) & (val['num_bicycle'] == 0))\n",
    "test_count = np.sum((test['num_vehicles'] > 1) & (test['num_bicycle'] == 0))\n",
    "total = train_count + val_count + test_count\n",
    "\n",
    "print (\"%.2f, %.2f, %.2f\" % (train_count/total*100, val_count/total*100, test_count/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onehotdf.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvfile = open(path+'features.csv', 'w')\n",
    "csvwriter = csv.writer(csvfile)\n",
    "for item in df.columns.values:\n",
    "    if '_nan' not in item:\n",
    "        csvwriter.writerow([item])\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bike = onehotdf[onehotdf['num_bicycle'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one = onehotdf[(onehotdf['num_vehicles'] == 1) & (onehotdf['num_bicycle'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multi = onehotdf[(onehotdf['num_vehicles'] > 1) & (onehotdf['num_bicycle'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(path+\"data_for_training/v4/split/bike.pkl\", 'wb') as outfile:\n",
    "    pickle.dump(bike, outfile)\n",
    "with open(path+\"data_for_training/v4/split/one.pkl\", 'wb') as outfile:\n",
    "    pickle.dump(one, outfile)\n",
    "with open(path+\"data_for_training/v4/split/multi.pkl\", 'wb') as outfile:\n",
    "    pickle.dump(multi, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onehotdf.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
