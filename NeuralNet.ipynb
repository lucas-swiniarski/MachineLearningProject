{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2a3d2eb7c8>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "manualSeed = 1 # fix seed\n",
    "print(\"Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameters of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 64\n",
    "nhidden = 300 # Hidden Layer Size\n",
    "nepochs = 10\n",
    "lr = .0001\n",
    "beta1 = .5 # Adam beta1 parameter\n",
    "noise = .1\n",
    "dropout = .65\n",
    "adam = True\n",
    "workers = 1 # Number of cores for loading data\n",
    "inputSize = 217\n",
    "log_interval = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Datasets And create Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "\n",
    "def loaderize(data_X, data_Y, balance):\n",
    "    # We want to bal\n",
    "    tensor_data_set = torch.utils.data.TensorDataset(torch.from_numpy(data_X).float(), torch.from_numpy(data_Y))\n",
    "    if balance:\n",
    "        # We increase probability of minority class, and decrease probability of dominant class so in average\n",
    "        # We sample the same amount of 1s and 0s even though classes are not balanced.\n",
    "        proba_1 = data_Y.mean()\n",
    "        weights = np.where(data_Y == 1., .5/proba_1, .5/(1.-proba_1))\n",
    "        sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, data_Y.shape[0])\n",
    "        return torch.utils.data.DataLoader(tensor_data_set, batch_size=batchSize, sampler=sampler, num_workers=int(workers))\n",
    "    return torch.utils.data.DataLoader(tensor_data_set, batch_size=batchSize, shuffle=True, num_workers=int(workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainloader = loaderize(pkl.load(open(path+'train_X.pkl','rb')), pkl.load(open(path+'train_y.pkl','rb')).values, True)\n",
    "valloader = loaderize(pkl.load(open(path+'val_X.pkl','rb')), pkl.load(open(path+'val_y.pkl','rb')).values, False)\n",
    "testloader = loaderize(pkl.load(open(path+'test_X.pkl','rb')), pkl.load(open(path+'test_y.pkl','rb')).values, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputSize, nhidden)\n",
    "        self.fc2 = nn.Linear(nhidden, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = F.relu(self.fc1(input))\n",
    "        input = self.dropout(input)\n",
    "        input = F.sigmoid(self.fc2(input))\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "\n",
    "if adam:\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr, betas = (beta1, 0.999))\n",
    "else:\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr = lr)\n",
    "    \n",
    "input = torch.FloatTensor(batchSize, inputSize)\n",
    "label = torch.LongTensor(batchSize)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "input = Variable(input)\n",
    "label = Variable(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(trainloader, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for i, (data, target) in enumerate(trainloader, 0):\n",
    "        input.data.resize_(data.size()).copy_(data)\n",
    "        label.data.resize_(target.size()).copy_(target)\n",
    "        model.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('[%d/%d] [%d/%d] Train Loss : %.4f' % \n",
    "                  (epoch, nepochs, \n",
    "                   i, len(trainloader), \n",
    "                    loss.data[0]))\n",
    "\n",
    "def test(testloader, epoch, isVal):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    all_labels = 0\n",
    "    all_preds = 0\n",
    "    \n",
    "    for i, (data, target) in enumerate(testloader, 0):\n",
    "        input.data.resize_(data.size()).copy_(data)\n",
    "        label.data.resize_(target.size()).copy_(target)\n",
    "        output = model(input)\n",
    "        test_loss += criterion(output, label)\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(label.data).cpu().sum()\n",
    "        if not torch.is_tensor(all_labels):\n",
    "            all_labels = target\n",
    "            all_preds = output.data[:,1]\n",
    "        else:\n",
    "            all_labels = torch.cat((all_labels, target), 0)\n",
    "            all_preds = torch.cat((all_preds, output.data[:,1]), 0)\n",
    "        \n",
    "    test_loss /= len(testloader)\n",
    "    \n",
    "    auc = metrics.roc_auc_score(all_labels.numpy(), all_preds.numpy())\n",
    "    if isVal:\n",
    "        print('\\n [%d/%d] ||VAL|| Average loss: %.4f, Accuracy: %d / %d (%.1f) AUC : %.6f \\n' % (\n",
    "                epoch, nepochs,\n",
    "                test_loss.data[0],\n",
    "                correct, len(testloader.dataset), 100. * correct / len(testloader.dataset), auc)\n",
    "             )\n",
    "    else:\n",
    "        print('\\n [%d/%d] ||TEST|| Average loss: %.4f, Accuracy: %d / %d (%.1f) AUC : %.6f \\n' % (\n",
    "                epoch, nepochs,\n",
    "                test_loss.data[0],\n",
    "                correct, len(testloader.dataset), 100. * correct / len(testloader.dataset), auc)\n",
    "             )\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] [0/8678] Train Loss : 0.6988\n",
      "[1/10] [1000/8678] Train Loss : 0.5897\n",
      "[1/10] [2000/8678] Train Loss : 0.5249\n",
      "[1/10] [3000/8678] Train Loss : 0.6046\n",
      "[1/10] [4000/8678] Train Loss : 0.5770\n",
      "[1/10] [5000/8678] Train Loss : 0.6476\n",
      "[1/10] [6000/8678] Train Loss : 0.5515\n",
      "[1/10] [7000/8678] Train Loss : 0.6581\n",
      "[1/10] [8000/8678] Train Loss : 0.5525\n",
      "\n",
      " [1/10] ||VAL|| Average loss: 0.5576, Accuracy: 125002 / 158523 (78.9) AUC : 0.7655 \n",
      "\n",
      "[2/10] [0/8678] Train Loss : 0.5935\n",
      "[2/10] [1000/8678] Train Loss : 0.5290\n",
      "[2/10] [2000/8678] Train Loss : 0.6046\n",
      "[2/10] [3000/8678] Train Loss : 0.5317\n",
      "[2/10] [4000/8678] Train Loss : 0.5285\n",
      "[2/10] [5000/8678] Train Loss : 0.5881\n",
      "[2/10] [6000/8678] Train Loss : 0.5581\n",
      "[2/10] [7000/8678] Train Loss : 0.4926\n",
      "[2/10] [8000/8678] Train Loss : 0.5376\n",
      "\n",
      " [2/10] ||VAL|| Average loss: 0.5675, Accuracy: 123522 / 158523 (77.9) AUC : 0.7669 \n",
      "\n",
      "[3/10] [0/8678] Train Loss : 0.5154\n",
      "[3/10] [1000/8678] Train Loss : 0.5603\n"
     ]
    }
   ],
   "source": [
    "val_loss_stored = np.inf\n",
    "\n",
    "for epoch in range(1, nepochs + 1):\n",
    "    train(trainloader, epoch)\n",
    "    val_loss = test(valloader, epoch, True)\n",
    "    if val_loss > val_loss_stored:\n",
    "        lr /= 2\n",
    "    val_loss_stored = val_loss\n",
    "test(testloader, epoch, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
