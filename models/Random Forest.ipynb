{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = \"../../../Google Drive/Data_science/NYU/Machine Learning/ML Project (Collisions)/\" #Joe\n",
    "path = \"../../../../Google Drive/ML Project (Collisions)/\" # Joyce\n",
    "# path = \"\" # Lucas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using H2O without 1 hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(path+\"data_for_training/v4/collisions_no1hot.pkl\", 'rb') as infile:\n",
    "    df_no1hot = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_no1hot = df_no1hot.sort_values('date_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "view_date = pd.to_datetime(df_no1hot['date_time'])\n",
    "\n",
    "train_indices = (0, np.sum(view_date < datetime.date(2015,9,12))-1)\n",
    "val_indices = (train_indices[1]+1,\\\n",
    "               train_indices[1] + \\\n",
    "               np.sum((view_date >= datetime.date(2015,9,12)) & (view_date < datetime.date(2016,7,31))))\n",
    "test_indices = (val_indices[1]+1,\\\n",
    "               val_indices[1] + np.sum(view_date >= datetime.date(2016,7,31)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_no1hot.to_csv(path+\"data_for_training/v2/no1hot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.init(ip=\"127.0.0.1\",max_mem_size_GB = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataFrame = h2o.upload_file(path=path+\"data_for_training/v2/no1hot.csv\", destination_frame=\"no1hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame = dataFrame.drop(['injured', 'killed', 'C1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = dataFrame[train_indices[0]:train_indices[1], :]\n",
    "val_df = dataFrame[val_indices[0]:val_indices[1], :]\n",
    "test_df = dataFrame[test_indices[0]:test_indices[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_confusion(confusion):\n",
    "    sensitivity = confusion[1][1]/float(np.sum(confusion[1]))\n",
    "    specificity = confusion[0][0]/float(np.sum(confusion[0]))\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_trees = [100, 200, 400]\n",
    "max_depth = [20, 30, 50, 100]\n",
    "min_rows = [5, 10, 50, 100]\n",
    "\n",
    "for i in itertools.product(n_trees,max_depth,min_rows):\n",
    "    \n",
    "    if i not in [i[0] for i in grid_search]:\n",
    "    \n",
    "        rf = H2ORandomForestEstimator(\n",
    "        model_id = \"rf\",\n",
    "        ntrees=i[0],\n",
    "        max_depth = i[1],\n",
    "        min_rows = i[2],\n",
    "        stopping_rounds=2)\n",
    "\n",
    "        rf.train(train_df.drop('injured_or_killed').columns, 'injured_or_killed',\\\n",
    "                 training_frame=train_df, validation_frame=val_df)\n",
    "\n",
    "        results = [i,\\\n",
    "                    rf.auc(train=True),\\\n",
    "                    rf.auc(valid=True),\\\n",
    "                    score_confusion(rf.confusion_matrix(train=True).to_list()),\\\n",
    "                    score_confusion(rf.confusion_matrix(valid=True).to_list())]\n",
    "        print (\"%s finished!\" % (str(i)))\n",
    "        print (\"Train AUC: %s, Valid AUC: %s, Train S&S: %s, Valid S&S: %s\" % (results[1], results[2], results[3], results[4]))\n",
    "        \n",
    "        grid_search.append(results)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_search)\n",
    "results.to_csv(\"tmp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn with 1 hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_variable = 'injured_or_killed'\n",
    "column_names = [i for i in pickle.load(open(path+'data_for_training/v4/collisions_1hot.pkl', 'rb')).columns.values if i != target_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sk_grid_search = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_trees = [200] #[50, 100, 200]\n",
    "max_depth = [50]#[10, 20, 50, 100]\n",
    "min_rows = [50] #[1, 10, 50, 100]\n",
    "\n",
    "for i in itertools.product(n_trees,max_depth,min_rows):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=i[0], max_depth=i[1], min_samples_leaf=i[2])\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    #Train results\n",
    "    t_predictions = rf.predict(X_train)\n",
    "    t_predictions_prob = [i[1] for i in rf.predict_proba(X_train)]\n",
    "    t_auc = metrics.roc_auc_score(y_train, t_predictions_prob)\n",
    "    t_sens = y_train['injured_or_killed'].astype(int).dot(t_predictions.astype(int))/np.sum(y_train)\n",
    "    t_spec = (y_train['injured_or_killed'] == 0).astype(int).dot((t_predictions==0).astype(int))/np.sum((y_train == 0))\n",
    "    \n",
    "    #Val results\n",
    "    v_predictions = rf.predict(X_val)\n",
    "    v_predictions_prob = [i[1] for i in rf.predict_proba(X_val)]\n",
    "    v_auc = metrics.roc_auc_score(y_val, v_predictions_prob)\n",
    "    v_sens = y_val['injured_or_killed'].astype(int).dot(v_predictions.astype(int))/np.sum(y_val)\n",
    "    v_spec = (y_val['injured_or_killed'] == 0).astype(int).dot((v_predictions==0).astype(int))/np.sum((y_val == 0))\n",
    "    \n",
    "    results = [i,\\\n",
    "                t_auc,\\\n",
    "                v_auc,\\\n",
    "                (t_sens, t_spec),\\\n",
    "                (v_sens, v_spec)]\n",
    "    \n",
    "    print (\"%s\\t%s\\t%s\\t%s\\t%s\" % (str(i), results[1], results[2], results[3], results[4]))\n",
    "    \n",
    "    sk_grid_search.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(sk_grid_search)\n",
    "results.to_csv(\"tmp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = H2ORandomForestEstimator(\n",
    "model_id = \"rf\",\n",
    "ntrees=200,\n",
    "max_depth = 50,\n",
    "min_rows = 50,\n",
    "stopping_rounds=2)\n",
    "\n",
    "rf.train(train_df.drop('injured_or_killed').columns, 'injured_or_killed',\\\n",
    "         training_frame=train_df, validation_frame=val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "num_features = 30\n",
    "\n",
    "y_pos = np.arange(num_features)\n",
    "\n",
    "plt.bar(y_pos, rf.varimp(True)['percentage'][:num_features])\n",
    "plt.xticks(y_pos, rf.varimp(True)['variable'][:num_features])\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, max_depth=50, min_samples_leaf=50)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(path+\"data_for_training/v4/collisions_1hot.pkl\", 'rb') as infile:\n",
    "    df = pickle.load(infile)\n",
    "\n",
    "view_date = pd.to_datetime(df['date_time'])\n",
    "\n",
    "train_indices = (0, np.sum(view_date < datetime.date(2015,9,12))-1)\n",
    "val_indices = (train_indices[1]+1,train_indices[1] + np.sum((view_date >= datetime.date(2015,9,12)) & (view_date < datetime.date(2016,7,31))))\n",
    "test_indices = (val_indices[1]+1, val_indices[1] + np.sum(view_date >= datetime.date(2016,7,31)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = df.iloc[train_indices[0]:train_indices[1]].drop(['injured_or_killed'], axis=1)\n",
    "y_train = df.iloc[train_indices[0]:train_indices[1]]['injured_or_killed']\n",
    "X_val = df.iloc[val_indices[0]:val_indices[1]].drop(['injured_or_killed'], axis=1)\n",
    "y_val = df.iloc[val_indices[0]:val_indices[1]]['injured_or_killed']\n",
    "X_test = df.iloc[test_indices[0]:test_indices[1]].drop(['injured_or_killed'], axis=1)\n",
    "y_test = df.iloc[test_indices[0]:test_indices[1]]['injured_or_killed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf = {}\n",
    "\n",
    "title_map = {'all_None.pkl': 'all collisions',\n",
    "            'bike_None.pkl': 'collisions involving a bicycle',\n",
    "            'one_None.pkl': 'collisions with one vehicle (no bicycles)',\n",
    "            'multi_None.pkl': 'collisions with multiple vehicles (no bicycles)'}\n",
    "\n",
    "for file in ['all_None.pkl', 'bike_None.pkl', 'one_None.pkl', 'multi_None.pkl']:\n",
    "    with open(os.path.join(path+\"models/\", file), 'rb') as infile:\n",
    "        rf[file] = pickle.load(infile)\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    num_features = 20\n",
    "    y_pos = np.arange(num_features)\n",
    "\n",
    "    importances = pd.Series(rf[file].feature_importances_, index=column_names)\n",
    "    topX = importances.sort_values(ascending = False)[:num_features]\n",
    "    plt.bar(y_pos, topX)\n",
    "    plt.xticks(y_pos, topX.index)\n",
    "    plt.title(\"Random Forest - Feature importances for model using {0}\".format(title_map[file]))\n",
    "    fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for model in ['all_None.pkl', 'all_balanced.pkl']:\n",
    "    pickle.dump(rf[model].predict_proba(X_val)[:,1],\n",
    "                open(path+\"ROC curve/{0}\".format(model), 'wb'),\n",
    "                pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from treeinterpreter import treeinterpreter as ti\n",
    "\n",
    "prediction, bias, contributions = ti.predict(rf['all_None.pkl'], X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
